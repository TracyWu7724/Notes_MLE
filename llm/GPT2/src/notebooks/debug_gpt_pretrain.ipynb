{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "PyTorch version: 2.6.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys                                                                                                                                                                                             \n",
    "sys.path.insert(0, os.path.join(os.path.dirname(\"__file__\"), \"..\"))  # add src/ to path\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "import importlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.nn import RMSNorm\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import bz2\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "import time\n",
    "\n",
    "import models.gpt2 as gpt2\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95cc747",
   "metadata": {},
   "source": [
    "## Tokenizer Setup and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c554b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up tokenizer...\n",
      "\n",
      "Test text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Tokens: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]\n",
      "Decoded: 'The quick brown fox jumps over the lazy dog.'\n",
      "Vocabulary size: 50257\n",
      "\n",
      "Special token IDs:\n",
      "  <|user|>: 50259\n",
      "  <|assistant|>: 50260\n",
      "  <|end|>: 50261\n",
      "  <|system|>: 50258\n",
      "  <|pad|>: 50257\n",
      "\n",
      "Actual vocabulary size needed: 50262\n",
      "Difference from tokenizer vocab size: 5\n",
      "\n",
      "✅ Tokenizer setup complete!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Set up the tokenizer\n",
    "print(\"Setting up tokenizer...\")\n",
    "tokenizer = gpt2.setup_tokenizer()\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"\\nTest text: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: '{decoded}'\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Find special token IDs\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
    "print(f\"\\nSpecial token IDs:\")\n",
    "for token in special_tokens:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    print(f\"  {token}: {token_id}\")\n",
    "\n",
    "# Calculate actual vocabulary size needed\n",
    "max_token_id = max(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "actual_vocab_size = max_token_id + 1\n",
    "print(f\"\\nActual vocabulary size needed: {actual_vocab_size}\")\n",
    "print(f\"Difference from tokenizer vocab size: {actual_vocab_size - tokenizer.vocab_size}\")\n",
    "\n",
    "print(\"\\n✅ Tokenizer setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77391e64",
   "metadata": {},
   "source": [
    "## GPTEmbedding Layer Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72094ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token IDs shape: torch.Size([2, 6])\n",
      "Sample token IDs: tensor([714, 792, 689,  27, 593, 683])\n",
      "\n",
      "Testing GPTEmbedding layer...\n",
      "Output shape: torch.Size([2, 6, 8])\n",
      "Expected shape: (2, 6, 8)\n",
      "Output sample (first token): tensor([ 0.3039, -1.1265, -0.6162,  0.0638,  0.5015], grad_fn=<SliceBackward0>)\n",
      "✅ Different tokens produce different embeddings\n",
      "\n",
      "✅ GPTEmbedding layer test passed!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "vocab_size = 1000\n",
    "emb_dim = 8\n",
    "context_length = 256\n",
    "batch_size = 2\n",
    "seq_length = 6\n",
    "\n",
    "# Create random token IDs\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "print(f\"Input token IDs shape: {token_ids.shape}\")\n",
    "print(f\"Sample token IDs: {token_ids[0]}\")\n",
    "\n",
    "# Initialize and test the embedding layer\n",
    "print(\"\\nTesting GPTEmbedding layer...\")\n",
    "embedding_layer = gpt2.GPTEmbedding(vocab_size, emb_dim, context_length)\n",
    "output = embedding_layer(token_ids)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_length, emb_dim)}\")\n",
    "print(f\"Output sample (first token): {output[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (batch_size, seq_length, emb_dim), \\\n",
    "    f\"Expected output shape {(batch_size, seq_length, emb_dim)}, got {output.shape}\"\n",
    "\n",
    "# Check that embeddings are different for different tokens\n",
    "if not torch.allclose(output[0, 0], output[0, 1]):\n",
    "    print(\"✅ Different tokens produce different embeddings\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: Different tokens produce similar embeddings\")\n",
    "\n",
    "print(\"\\n✅ GPTEmbedding layer test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0b041",
   "metadata": {},
   "source": [
    "## Model Component Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "156342ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 7, 16])\n",
      "\n",
      "Testing MultiHeadAttention with RoPE...\n",
      "Output shape: torch.Size([3, 7, 16])\n",
      "Expected shape: (3, 7, 16)\n",
      "Output sample: tensor([-0.2015,  0.3772,  0.2905, -0.0066, -0.2134], grad_fn=<SliceBackward0>)\n",
      ":white_check_mark: RoPE working: outputs differ for same tokens at different positions\n",
      "✅ RoPE is working: same tokens at different positions produce different outputs\n",
      "\n",
      "✅ MultiHeadAttention with RoPE test passed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Test MultiHeadAttention with RoPE\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "torch.manual_seed(123)  # For reproducible results\n",
    "d_in = 16\n",
    "d_out = d_in\n",
    "num_heads = 4\n",
    "num_kv_heads = 2\n",
    "context_length = 32\n",
    "dropout = 0.0\n",
    "batch_size = 3\n",
    "seq_len = 7\n",
    "\n",
    "# Create random input tensor\n",
    "x = torch.randn(batch_size, seq_len, d_in)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Initialize MultiHeadAttention with RoPE\n",
    "print(\"\\nTesting MultiHeadAttention with RoPE...\")\n",
    "mha = gpt2.MultiHeadAttention(d_in=d_in, max_seq_length=context_length, dropout=dropout, num_heads=num_heads, num_kv_heads=num_kv_heads, bias_qkv=True)\n",
    "out = mha(x)[0]\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, d_out)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, d_out), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, d_out)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test RoPE positional sensitivity\n",
    "seq = torch.randn(1, 2, d_in)\n",
    "seq_shifted = torch.cat([torch.zeros(1, 3, d_in), seq], dim=1)  # same tokens at later positions\n",
    "\n",
    "out1 = mha(seq)[0]\n",
    "out2 = mha(seq_shifted)[0][:, -2:, :]\n",
    "\n",
    "if not torch.allclose(out1, out2, atol=1e-5):\n",
    "    print(\":white_check_mark: RoPE working: outputs differ for same tokens at different positions\")\n",
    "else:\n",
    "    print(\":warning: RoPE may not be applied correctly\")\n",
    "\n",
    "# The outputs should be different due to RoPE encoding different positions\n",
    "if not torch.allclose(out1, out2):\n",
    "    print(\"✅ RoPE is working: same tokens at different positions produce different outputs\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: RoPE might not be working correctly\")\n",
    "\n",
    "print(\"\\n✅ MultiHeadAttention with RoPE test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843080bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8, 16])\n",
      "Input sample: tensor([-0.2807, -0.3287,  0.4688,  1.5427, -1.2366])\n",
      "\n",
      "Testing SwiGLU activation...\n",
      "Output shape: torch.Size([4, 8, 16])\n",
      "Expected shape: (4, 8, 16)\n",
      "Output sample: tensor([-0.0409, -0.0312, -0.1020,  0.0339,  0.0536], grad_fn=<SliceBackward0>)\n",
      "✅ SwiGLU is non-linear (as expected)\n",
      "\n",
      "✅ SwiGLU activation test passed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Test SwiGLU Activation Function\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "d_model = 16\n",
    "d_ff = 32\n",
    "batch_size = 4\n",
    "seq_len = 8\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input sample: {x[0, 0, :5]}\")\n",
    "\n",
    "# Initialize and test SwiGLU\n",
    "print(\"\\nTesting SwiGLU activation...\")\n",
    "swiglu = gpt2.SwiGLU(d_model, d_ff, d_model)\n",
    "out = swiglu(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, d_model)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, d_model), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, d_model)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that SwiGLU is non-linear\n",
    "# Create two different inputs\n",
    "x1 = torch.randn(1, 1, d_model)\n",
    "x2 = torch.randn(1, 1, d_model)\n",
    "out1 = swiglu(x1)\n",
    "out2 = swiglu(x2)\n",
    "\n",
    "# Test linearity: SwiGLU(x1 + x2) should NOT equal SwiGLU(x1) + SwiGLU(x2)\n",
    "combined_input = x1 + x2\n",
    "combined_output = swiglu(combined_input)\n",
    "sum_outputs = out1 + out2\n",
    "\n",
    "if not torch.allclose(combined_output, sum_outputs):\n",
    "    print(\"✅ SwiGLU is non-linear (as expected)\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: SwiGLU appears to be linear\")\n",
    "\n",
    "print(\"\\n✅ SwiGLU activation test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ef9ba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 4, 16])\n",
      "Input sample: tensor([-0.2383, -0.4250, -0.7056, -0.7724,  0.2178])\n",
      "\n",
      "Testing FeedForward layer...\n",
      "Output shape: torch.Size([10, 4, 16])\n",
      "Expected shape: (10, 4, 16)\n",
      "Output sample: tensor([ 0.1601,  0.0216, -0.1432,  0.0224,  0.2945], grad_fn=<SliceBackward0>)\n",
      "✅ FeedForward transforms the input (as expected)\n",
      "\n",
      "✅ FeedForward layer test passed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Test FeedForward Layer\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test parameters\n",
    "emb_dim = 16\n",
    "batch_size = 10\n",
    "seq_len = 4\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, emb_dim)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Input sample: {x[0, 0, :5]}\")\n",
    "\n",
    "# Initialize and test FeedForward\n",
    "print(\"\\nTesting FeedForward layer...\")\n",
    "ff = gpt2.FeedForward(emb_dim)\n",
    "out = ff(x)\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Expected shape: {(batch_size, seq_len, emb_dim)}\")\n",
    "print(f\"Output sample: {out[0, 0, :5]}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert out.shape == (batch_size, seq_len, emb_dim), \\\n",
    "    f\"Expected output shape {(batch_size, seq_len, emb_dim)}, got {out.shape}\"\n",
    "assert not torch.isnan(out).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that FeedForward transforms the input\n",
    "if not torch.allclose(x, out):\n",
    "    print(\"✅ FeedForward transforms the input (as expected)\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: FeedForward doesn't seem to transform the input\")\n",
    "\n",
    "print(\"\\n✅ FeedForward layer test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774a60ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Testing TransformerBlock...\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "Expected shape: (2, 4, 768)\n",
      "✅ TransformerBlock transforms the input (as expected)\n",
      "\n",
      "✅ TransformerBlock test passed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Test TransformerBlock\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test configuration\n",
    "OG_GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# Create test input\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, OG_GPT_CONFIG[\"emb_dim\"])\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Initialize and test TransformerBlock\n",
    "print(\"\\nTesting TransformerBlock...\")\n",
    "block = gpt2.TransformerBlock(OG_GPT_CONFIG)\n",
    "output = block(x)[0]\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(2, 4, OG_GPT_CONFIG['emb_dim'])}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (2, 4, OG_GPT_CONFIG[\"emb_dim\"]), \\\n",
    "    f\"Expected output shape {(2, 4, OG_GPT_CONFIG['emb_dim'])}, got {output.shape}\"\n",
    "assert not torch.isnan(output).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Test that the block transforms the input\n",
    "if not torch.allclose(x, output):\n",
    "    print(\"✅ TransformerBlock transforms the input (as expected)\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: TransformerBlock doesn't seem to transform the input\")\n",
    "\n",
    "print(\"\\n✅ TransformerBlock test passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cb2b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size: 50262\n",
      "Input sentence: 'The quick brown fox jumps over the lazy dog.'\n",
      "Token IDs: tensor([  464,  2068,  7586, 21831, 18045,   625,   262, 16931,  3290,    13])\n",
      "Token IDs shape: torch.Size([1, 10])\n",
      "\n",
      "Testing GPTModel...\n",
      "Output shape: torch.Size([1, 10, 50262])\n",
      "Expected shape: (1, 10, 50262)\n",
      "Logits variance: 515.8440\n",
      "✅ Logits have reasonable variance\n",
      "\n",
      "✅ GPTModel test passed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Test Complete GPTModel\n",
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Calculate vocabulary size from tokenizer\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\", \"<|end|>\", \"<|system|>\", \"<|pad|>\"]\n",
    "max_token_id = max(tokenizer.convert_tokens_to_ids(token) for token in special_tokens)\n",
    "actual_vocab_size = max_token_id + 1\n",
    "\n",
    "# Test configuration\n",
    "CUSTOM_GPT_CONFIG = {\n",
    "    \"vocab_size\": actual_vocab_size,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 512,\n",
    "    \"n_heads\": 8,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "print(f\"Using vocabulary size: {actual_vocab_size}\")\n",
    "\n",
    "# Test with real tokenized text\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "token_ids = tokenizer.encode(sentence)\n",
    "token_ids = torch.tensor(token_ids)\n",
    "print(f\"Input sentence: '{sentence}'\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Token IDs shape: {token_ids.unsqueeze(0).shape}\")\n",
    "\n",
    "# Initialize and test GPTModel\n",
    "print(\"\\nTesting GPTModel...\")\n",
    "gpt_model = gpt2.GPT(CUSTOM_GPT_CONFIG)\n",
    "output = gpt_model(token_ids.unsqueeze(0))[0]\n",
    "\n",
    "# Verify output\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected shape: {(1, len(token_ids), actual_vocab_size)}\")\n",
    "\n",
    "# Sanity checks\n",
    "assert output.shape == (1, len(token_ids), actual_vocab_size), \\\n",
    "    f\"Expected output shape {(1, len(token_ids), actual_vocab_size)}, got {output.shape}\"\n",
    "assert not torch.isnan(output).any(), \"Output contains NaNs!\"\n",
    "\n",
    "# Check that logits are reasonable (not all the same)\n",
    "logits_variance = output.var()\n",
    "print(f\"Logits variance: {logits_variance:.4f}\")\n",
    "if logits_variance > 0.01:\n",
    "    print(\"✅ Logits have reasonable variance\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: Logits have very low variance\")\n",
    "\n",
    "print(\"\\n✅ GPTModel test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f091528",
   "metadata": {},
   "source": [
    "## Text Generation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ebc10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting context: 'The quick brown fox'\n",
      "Output: tensor([[  464,  2068,  7586, 21831, 21831, 21831, 21831, 21831, 21831, 21831,\n",
      "         21831, 21831, 21831, 21831]])\n",
      "Output length: 14\n",
      "Generated text: 'The quick brown fox fox fox fox fox fox fox fox fox fox fox'\n",
      "\n",
      "Note: The generated text will be random since the model is untrained.\n",
      "This is expected! After training, the model should generate more coherent text.\n",
      "\n",
      "✅ Text generation test passed!\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(gpt2)  # Reload to get latest changes\n",
    "\n",
    "# Test text generation\n",
    "start_context = \"The quick brown fox\"\n",
    "print(f\"Starting context: '{start_context}'\")\n",
    "\n",
    "# Generate text (will be random since model is untrained)\n",
    "full_text = gpt2.generate_text(\n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer,\n",
    "    model=gpt_model,\n",
    "    max_new_tokens=10,\n",
    "    context_size=CUSTOM_GPT_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(f\"Generated text: '{full_text}'\")\n",
    "print(\"\\nNote: The generated text will be random since the model is untrained.\")\n",
    "print(\"This is expected! After training, the model should generate more coherent text.\")\n",
    "\n",
    "print(\"\\n✅ Text generation test passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc2f36",
   "metadata": {},
   "source": [
    "## Dataset Creation Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441a749",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911652e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c09fd",
   "metadata": {},
   "source": [
    "## DataLoader Creation Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d02acb",
   "metadata": {},
   "source": [
    "## Dataset Creating Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49bf68",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
